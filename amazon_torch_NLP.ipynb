{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55e9ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries\n",
    "# Always start by importing all necessary libraries for data handling, processing, and modeling.\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac6c739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dataset entry: {'label': 1, 'text': 'Stuning even for the non-gamer\\n\\nThis sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^', 'label_text': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load Dataset\n",
    "# Download the Amazon Polarity dataset for sentiment analysis (positive/negative review classification).\n",
    "dataset = load_dataset(\"mteb/amazon_polarity\", split=\"train\")\n",
    "print(\"Sample dataset entry:\", dataset[0])  # Inspect sample data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3631f10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['label', 'text', 'label_text'])\n"
     ]
    }
   ],
   "source": [
    "# STep 2.1: Check keys in the dataset\n",
    "print(dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "110aec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocess Data\n",
    "# Extract review texts and their labels, preparing lists for later tokenization and encoding.\n",
    "texts = [sample[\"text\"] for sample in dataset]\n",
    "labels = [sample[\"label\"] for sample in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97816e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Split Data into Training and Validation Sets\n",
    "texts_train, texts_val, labels_train, labels_val = train_test_split(\n",
    "    texts, labels, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48644fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Tokenization & Vocabulary\n",
    "# For basic models (like LSTM), tokenize each review using a simple tokenizer.\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, texts_train))\n",
    "vocab.set_default_index(0)  # Handle unknown tokens gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ac380f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Custom PyTorch Dataset Class\n",
    "# Define a PyTorch Dataset to process and encode each review as tensors for training.\n",
    "class AmazonReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, vocab, max_length=100):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenizer(self.texts[idx])\n",
    "        indices = [self.vocab[token] for token in tokens][:self.max_length]\n",
    "        padding = [0] * (self.max_length - len(indices))\n",
    "        return torch.tensor(indices + padding), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66128fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: DataLoader Setup\n",
    "# Create DataLoaders for training and validation sets\n",
    "train_dataset = AmazonReviewDataset(texts_train, labels_train, tokenizer, vocab)\n",
    "val_dataset = AmazonReviewDataset(texts_val, labels_val, tokenizer, vocab)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d05b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Model Definition (Simple LSTM)\n",
    "# Build a simple neural network using PyTorch, suitable for sequence modeling tasks like text classification.\n",
    "class SimpleLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # Use the last output for classification\n",
    "        return self.fc(out)\n",
    "\n",
    "model = SimpleLSTMClassifier(vocab_size=len(vocab), embed_dim=128, hidden_dim=128, output_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "738f35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Training Setup\n",
    "# Set up optimizer and loss function for model training.\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42227856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Training Loop\n",
    "# Train the model over multiple epochs.\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f77bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Evaluation\n",
    "# Evaluate model on validation data and calculate accuracy.\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e924145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Save Model\n",
    "# To deploy, save the trained model using torch.save()\n",
    "torch.save(model.state_dict(), \"amazon_polarity_lstm_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec5270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model state\n",
    "model_loaded = SimpleLSTMClassifier(vocab_size=len(vocab), embed_dim=128, hidden_dim=128, output_dim=2)\n",
    "model_loaded.load_state_dict(torch.load(\"amazon_polarity_lstm_model.pth\"))\n",
    "model_loaded.eval()\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    tokens = tokenizer(text)\n",
    "    indices = [vocab[token] for token in tokens][:100]\n",
    "    padding = [0] * (100 - len(indices))\n",
    "    input_tensor = torch.tensor(indices + padding).unsqueeze(0)  # Add batch dim\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model_loaded(input_tensor)\n",
    "        prediction = torch.argmax(output, dim=1).item()\n",
    "    \n",
    "    return \"Positive\" if prediction == 1 else \"Negative\"\n",
    "\n",
    "# Example usage:\n",
    "sample_review = \"This product exceeded my expectations and works perfectly!\"\n",
    "print(f\"Review: {sample_review}\")\n",
    "print(f\"Predicted Sentiment: {predict_sentiment(sample_review)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
